<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>ReinforencementLearning-Prioritized_Experience_Replay | Blog for recording</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Paper: ICLR 2016 Motivation一般的Experience Replay池子中，所有的transition都是均匀采样抽取的. 但是所有的transition从重要程度上来讲，他们并不是都同等重要,有些经验更重要. eg:  整个环境中有n个状态，2个动作，在这其中，只有绿色的transition可以获得正向reward($1/2^n$概率)，而绝大多数红线所代表的trans">
<meta name="keywords" content="Reinforcement Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="ReinforencementLearning-Prioritized_Experience_Replay">
<meta property="og:url" content="https://666haiwen.github.io/2019/09/28/ReinforencementLearning-Prioritized-Experience-Replay/index.html">
<meta property="og:site_name" content="Blog for recording">
<meta property="og:description" content="Paper: ICLR 2016 Motivation一般的Experience Replay池子中，所有的transition都是均匀采样抽取的. 但是所有的transition从重要程度上来讲，他们并不是都同等重要,有些经验更重要. eg:  整个环境中有n个状态，2个动作，在这其中，只有绿色的transition可以获得正向reward($1/2^n$概率)，而绝大多数红线所代表的trans">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://666haiwen.github.io/2019/09/28/ReinforencementLearning-Prioritized-Experience-Replay/1.png">
<meta property="og:image" content="https://666haiwen.github.io/2019/09/28/ReinforencementLearning-Prioritized-Experience-Replay/2.png">
<meta property="og:image" content="https://666haiwen.github.io/2019/09/28/ReinforencementLearning-Prioritized-Experience-Replay/3.png">
<meta property="og:updated_time" content="2019-09-28T09:16:12.309Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ReinforencementLearning-Prioritized_Experience_Replay">
<meta name="twitter:description" content="Paper: ICLR 2016 Motivation一般的Experience Replay池子中，所有的transition都是均匀采样抽取的. 但是所有的transition从重要程度上来讲，他们并不是都同等重要,有些经验更重要. eg:  整个环境中有n个状态，2个动作，在这其中，只有绿色的transition可以获得正向reward($1/2^n$概率)，而绝大多数红线所代表的trans">
<meta name="twitter:image" content="https://666haiwen.github.io/2019/09/28/ReinforencementLearning-Prioritized-Experience-Replay/1.png">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/typing.css">
  <link rel="stylesheet" href="/css/donate.css">
  
</head>
</html>
  
    
      <body>
    
  
      <div id="container" class="container">
        <article id="post-ReinforencementLearning-Prioritized-Experience-Replay" class="article article-type-post" itemscope itemprop="blogPost">
  <header id="header" class="header">
  <nav class="mobile-nav">
    <h1 class="nickname">Chen Zexian</h1>
    <ul class="mobile-nav-menu">
      <label for="mobile-menu-toggle"><a>&#9776; Menu</a></label>
      <input type="checkbox" id="mobile-menu-toggle"/>
      <ul class="mobile-nav-link">
        
        <a href="/">Home</a>
        
        <a href="/archives">Archives</a>
        
        <a href="/categories/paper">Paper Reading</a>
        
        <a href="/categories/frontend">Frontend</a>
        
        <a href="/categories/visualization">Visualization</a>
        
        <a href="/categories/algorithm">Algorithm</a>
        
        <a href="/categories/daily">Daily</a>
        
      </ul>
    </ul>
  </nav>
	
		<nav id="main-nav" class="main-nav nav-left">
	
	
	  <a class="main-nav-link" href="/">Home</a>
	
	  <a class="main-nav-link" href="/archives">Archives</a>
	
	  <a class="main-nav-link" href="/categories/paper">Paper Reading</a>
	
	  <a class="main-nav-link" href="/categories/frontend">Frontend</a>
	
	  <a class="main-nav-link" href="/categories/visualization">Visualization</a>
	
	  <a class="main-nav-link" href="/categories/algorithm">Algorithm</a>
	
	  <a class="main-nav-link" href="/categories/daily">Daily</a>
	
  </nav>
</header>

  <hr/>
  <div class="article-inner">
    

    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      ReinforencementLearning-Prioritized_Experience_Replay
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Motivation"><span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#定义"><span class="toc-text">定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-prioritization"><span class="toc-text">Stochastic prioritization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法流程"><span class="toc-text">算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#线段树"><span class="toc-text">线段树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ref"><span class="toc-text">Ref</span></a></li></ol>
    </div>
    
        <p><a href="https://arxiv.org/pdf/1511.05952.pdf" target="_blank" rel="noopener">Paper</a>: ICLR 2016</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>一般的Experience Replay池子中，所有的transition都是均匀采样抽取的.</p>
<p>但是所有的transition从重要程度上来讲，他们并不是都<strong>同等重要</strong>,有些经验更重要.</p>
<p>eg:</p>
<p><img src="/2019/09/28/ReinforencementLearning-Prioritized-Experience-Replay/1.png" alt="1569659330757"></p>
<p>整个环境中有n个状态，2个动作，在这其中，只有绿色的transition可以获得正向reward($1/2^n$概率)，而绝大多数红线所代表的transition是不重要的.显然他们不是同等重要.</p>
<p>因此对于每一个transition，我们要优先学习那些<strong>更为重要的</strong>.</p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>那么，如何定义这个重要程度显得尤为关键,文章中所使用的是TD-error.</p>
<p>也就是说当我没产生一个transition之后，我都会赋予其一个权重，该权重与TD-error相关.（TD-error越大代表这个点我预测的越不准确，需要重点关注).</p>
<p>因为，DQN相关的算法是off-policy的算法，所以说每一个transition的权重并不是实时的，为了效率考虑，我只会修改我sample到的那些transition(Batch-size)的权重.</p>
<p>那么，怎么对其进行抽样呢(Batch size)</p>
<p><strong>1.贪心抽样</strong></p>
<p>贪心存在的问题是，</p>
<ul>
<li>我这个重要性本来就是不实时的，贪心要抽取的是全局下那个重要程度最高的，矛盾，所以效果会不好.</li>
<li>容易overfitting，在实验中很有可能我后面选择的那些transition就是那一小搓.</li>
</ul>
<p>因此，比较好的是随机化的采样过程(stochastic prioritization)，重要程度越高只代表概率越高。</p>
<h3 id="Stochastic-prioritization"><a href="#Stochastic-prioritization" class="headerlink" title="Stochastic prioritization"></a>Stochastic prioritization</h3><p><strong>Importance sampling</strong></p>
<p>因为采样顺序会改变样本分布，假设原来的样本$x$服从分布$A$，在改变采样顺序后，$x$服从的分布为$B$,</p>
<p>我们要求的是$\mathbb{E}_{x\sim A}[f(x)] = \mathbb{E}_{x\sim B}]$， </p>
<p>则：</p>
<script type="math/tex; mode=display">
\mathbb{E}_{x\sim A}[f(x)]=\sum_xP_A(x)f(x)=\sum_xP_B(x)\frac{P_A(x)}{P_B(x)}f(x)=\mathbb{E}_{x\sim B}\frac{P_A(x)}{P_B(x)}f(x)</script><p>所以，我们需要给每个重要程度再乘以一个系数$w$,$w_i=（\frac{P_A(x_i)}{P_B(x_i)})^\beta$</p>
<p>其中，$P_A(x) = 1/N;$   $P_B(x) = P(i)$(我们定义的概率), $\beta$用于调节bias的程度</p>
<p>而且，这个$w_i$最后要除以$max_iw_i$(归一化,而且是整个经验池的归一化)</p>
<p><strong>P(i)</strong></p>
<p>重要程度转为概率，就用softmax嘛</p>
<script type="math/tex; mode=display">
P(i)=\frac{p_i^\alpha}{\sum_kp_k^\alpha}</script><p>其中，$p_i$代表了第i个transition的重要程度(priority),$\alpha$用于调节优先程度($\alpha=0$时为均匀采样)</p>
<p>有两种不同的$p_i $定义方法</p>
<p><strong>·proportional prioritization</strong></p>
<ul>
<li>$pi=|δi|+ϵpi=|δi|+ϵ$，其中$δi$为TD-error，$ϵ$用于防止概率为$0$。</li>
<li>可能会<strong>对outlier更加敏感</strong>。</li>
<li>实现的时候采用sum tree的数据结构。</li>
</ul>
<p><strong>·rank-based prioritization</strong></p>
<ul>
<li>$pi=1/rank(i)$; $rank(i)$也是根据TD-error在整个memory中排序的结果</li>
<li>只是<strong>定性</strong>地考虑priority，没有定量地考虑priority。</li>
</ul>
<p><strong>实现</strong></p>
<p>用的是基于线段树进行抽样。</p>
<p>假设memory大小为100，batch size为5,那么将100等概率分成5个区间，每个区间的概率为$p_{[]}=\sum_kp_k/batch_size$ ,然后在每个区间中均匀采样出1个.</p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p><img src="/2019/09/28/ReinforencementLearning-Prioritized-Experience-Replay/2.png" alt="1569661118802"></p>
<p>openAI的官方代码：<a href="https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py" target="_blank" rel="noopener">https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py</a></p>
<h3 id="线段树"><a href="#线段树" class="headerlink" title="线段树"></a>线段树</h3><p>线段是是一个完全二叉树(满二叉树更好维护)，可以用一个一维数组来维护.</p>
<p>假设我有n个叶节点，那么我数组的长度为$capacity= 2^{k+1}$ where $k=min(2^k\ge n)$</p>
<p>性质为：叶节点就是一个无序的数组，除此之外，$node_k=f_{operation}(node_{left},node_{right})$</p>
<p>父节点的值为子结点值得某种operation后的结果，可以是(sum、max、min….)</p>
<p>eg:</p>
<p><img src="/2019/09/28/ReinforencementLearning-Prioritized-Experience-Replay/3.png" alt="1569661669501"></p>
<p>opeani的官方代码:<a href="https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py" target="_blank" rel="noopener">https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py</a></p>
<h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h3><p><a href="https://zhuanlan.zhihu.com/p/38358183" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38358183</a></p>
<p><a href="http://www.meltycriss.com/2018/03/18/paper-prioritized-experience-replay/" target="_blank" rel="noopener">http://www.meltycriss.com/2018/03/18/paper-prioritized-experience-replay/</a></p>
<p>线段树:<a href="https://www.jianshu.com/p/6fd130084a43" target="_blank" rel="noopener">https://www.jianshu.com/p/6fd130084a43</a></p>

      
      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">Published Date:</span>
          <a href="/2019/09/28/ReinforencementLearning-Prioritized-Experience-Replay/" class="article-date">
  <time datetime="2019-09-28T09:11:36.000Z" itemprop="datePublished">2019-09-28</time>
</a>

        </li>
        
          <li>
            <span class="label">Category:</span>
            
  <div class="article-category">
    <a class="article-category-link" href="/categories/algorithm/">algorithm</a>
  </div>


          </li>
        
        
          <li>
            <span class="label">Tag:</span>
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>


          </li>
        
        <hr/>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <span id="article-nav-newer" class="article-nav-link-wrap newer"></span>
  
  
    <a href="/2019/09/17/React-Usage/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">React-Usage</div>
    </a>
  
</nav>


  
</article>










      </div>
      
    <footer id="footer" class="post-footer footer">
      
      <hr/>
      <div id="footerContent" class="footer-content">
        <p>Do things you want.</p>


      </div>
    </footer>

      







<script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>


<script src="/js/typing.js"></script>
<!--[if lt IE 9]><script src="https://cdn.jsdelivr.net/npm/html5shiv@3/dist/html5shiv.min.js"></script><![endif]-->







    </div>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>
