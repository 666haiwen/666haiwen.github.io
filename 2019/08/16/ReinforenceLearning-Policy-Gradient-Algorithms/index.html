<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>ReinforenceLearning--Policy Gradient Algorithms | Blog for recording</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="PolicyPolicy $\pi$,代表在状态$s$，会执行的动作$a$.(分为确定性和随机性)  Deterministic: $\pi(s)=a.$ Stochastic: $\pi(a|s)=\mathbb{P}_\pi[A=a|S=s]$  $\pi_\theta(a|s)=\mathbb{P}_\pi[A=a|S=s,\theta]$   Policy Gradientobject f">
<meta name="keywords" content="Reinforcement Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="ReinforenceLearning--Policy Gradient Algorithms">
<meta property="og:url" content="https://666haiwen.github.io/2019/08/16/ReinforenceLearning-Policy-Gradient-Algorithms/index.html">
<meta property="og:site_name" content="Blog for recording">
<meta property="og:description" content="PolicyPolicy $\pi$,代表在状态$s$，会执行的动作$a$.(分为确定性和随机性)  Deterministic: $\pi(s)=a.$ Stochastic: $\pi(a|s)=\mathbb{P}_\pi[A=a|S=s]$  $\pi_\theta(a|s)=\mathbb{P}_\pi[A=a|S=s,\theta]$   Policy Gradientobject f">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://666haiwen.github.io/2019/08/16/ReinforenceLearning-Policy-Gradient-Algorithms/ac-1.png">
<meta property="og:image" content="https://666haiwen.github.io/2019/08/16/ReinforenceLearning-Policy-Gradient-Algorithms/ac-2.png">
<meta property="og:image" content="https://666haiwen.github.io/2019/08/16/ReinforenceLearning-Policy-Gradient-Algorithms/AC-3.png">
<meta property="og:image" content="https://666haiwen.github.io/2019/08/16/ReinforenceLearning-Policy-Gradient-Algorithms/AC-4.png">
<meta property="og:updated_time" content="2019-08-19T07:30:24.514Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ReinforenceLearning--Policy Gradient Algorithms">
<meta name="twitter:description" content="PolicyPolicy $\pi$,代表在状态$s$，会执行的动作$a$.(分为确定性和随机性)  Deterministic: $\pi(s)=a.$ Stochastic: $\pi(a|s)=\mathbb{P}_\pi[A=a|S=s]$  $\pi_\theta(a|s)=\mathbb{P}_\pi[A=a|S=s,\theta]$   Policy Gradientobject f">
<meta name="twitter:image" content="https://666haiwen.github.io/2019/08/16/ReinforenceLearning-Policy-Gradient-Algorithms/ac-1.png">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/typing.css">
  <link rel="stylesheet" href="/css/donate.css">
  
</head>
</html>
  
    
      <body>
    
  
      <div id="container" class="container">
        <article id="post-ReinforenceLearning-Policy-Gradient-Algorithms" class="article article-type-post" itemscope itemprop="blogPost">
  <header id="header" class="header">
  <nav class="mobile-nav">
    <h1 class="nickname">Chen Zexian</h1>
    <ul class="mobile-nav-menu">
      <label for="mobile-menu-toggle"><a>&#9776; Menu</a></label>
      <input type="checkbox" id="mobile-menu-toggle">
      <ul class="mobile-nav-link">
        
        <a href="/">Home</a>
        
        <a href="/archives">Archives</a>
        
        <a href="/categories/paper">Paper Reading</a>
        
        <a href="/categories/frontend">Frontend</a>
        
        <a href="/categories/visualization">Visualization</a>
        
        <a href="/categories/algorithm">Algorithm</a>
        
        <a href="/categories/daily">Daily</a>
        
      </ul>
    </ul>
  </nav>
	
		<nav id="main-nav" class="main-nav nav-left">
	
	
	  <a class="main-nav-link" href="/">Home</a>
	
	  <a class="main-nav-link" href="/archives">Archives</a>
	
	  <a class="main-nav-link" href="/categories/paper">Paper Reading</a>
	
	  <a class="main-nav-link" href="/categories/frontend">Frontend</a>
	
	  <a class="main-nav-link" href="/categories/visualization">Visualization</a>
	
	  <a class="main-nav-link" href="/categories/algorithm">Algorithm</a>
	
	  <a class="main-nav-link" href="/categories/daily">Daily</a>
	
  </nav>
</header>

  <hr>
  <div class="article-inner">
    

    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      ReinforenceLearning--Policy Gradient Algorithms
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Policy"><span class="toc-text">Policy</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Policy-Gradient"><span class="toc-text">Policy Gradient</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gt-policy-gradient-theorem"><span class="toc-text">==&gt;policy gradient theorem</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gt-李宏毅教授PPT"><span class="toc-text">==&gt;李宏毅教授PPT</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Actor-Critic"><span class="toc-text">Actor-Critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol>
    </div>
    
        <h3 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h3><p>Policy $\pi$,代表在状态$s$，会执行的动作$a$.(分为确定性和随机性)</p>
<ul>
<li>Deterministic: $\pi(s)=a.$</li>
<li><p>Stochastic: $\pi(a|s)=\mathbb{P}_\pi[A=a|S=s]$</p>
</li>
<li><p>$\pi_\theta(a|s)=\mathbb{P}_\pi[A=a|S=s,\theta]$</p>
</li>
</ul>
<h4 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h4><p><strong>object function</strong>:</p>
<p><strong>episodic environments:</strong></p>
<p>$J_1(\theta)=V^{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}[V_1]$</p>
<p><strong>continuing environments:</strong></p>
<p><em>连续型环境就没有初始状态一说了，那么就取平均，考虑agent在某时刻处于某个状态下的概率，即状态分布</em></p>
<p><strong>average value:</strong></p>
<p>$J_{avV}(\theta)=\sum_sd^{\pi_\theta}(s)V^{\pi_\theta}(s)$</p>
<p>对每个可能的状态计算从该时刻(该状态)开始于环境一直交互下去的奖励，然后对其概率分布求和.</p>
<p><strong>average reward per time-step:</strong></p>
<p>$J_{avR}(\theta)=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)\mathcal{R}_s^a$</p>
<p>其中,$d^{\pi_\theta}(s)​$是马尔科夫链在$\pi_\theta​$下的随机概率分布；即$d^\pi(s)=lim_{t→∞}P(s_t=s|s_0,\pi_θ)​$就表示你从状态$s_0​$开始，在策略$\pi_θ​$下经过$t​$个时间步后到达状态$s​$的概率。</p>
<p>该式代表了到达某个状态$s$,并且采用动作$a$的情况下，获得的平均回报$\mathcal{R_s^a}$,概率化累加就是平均回报.</p>
<p>这个可以如下计算(以李宏毅教授PPT为例)，每条迹的概率*该条迹累计的期望即为平均</p>
<p><img src="/2019/08/16/ReinforenceLearning-Policy-Gradient-Algorithms/ac-1.png" alt></p>
<p>目标是将目标函数最大化，可以用梯度下降法将其最大化</p>
<p><em>注：用基于梯度的策略优化时，是基于序列结构片段，如果无穷地和环境交互，得到一个结果是没有意义的；就是和环境交互中的某一个序列，将其拿出来进行学习，然后优化策略</em></p>
<p><strong>one-step MDP(per time-step):</strong></p>
<script type="math/tex; mode=display">
\begin{split}
J(\theta)&=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)\mathcal{R}_s^a  
\\
\nabla_\theta J(\theta)&=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)\nabla_\theta log\pi_\theta(s,a)\mathcal{R}_s^a
\end{split}</script><h4 id="gt-policy-gradient-theorem"><a href="#gt-policy-gradient-theorem" class="headerlink" title="==&gt;policy gradient theorem"></a>==&gt;policy gradient theorem</h4><p><img src="/2019/08/16/ReinforenceLearning-Policy-Gradient-Algorithms/ac-2.png" alt></p>
<h4 id="gt-李宏毅教授PPT"><a href="#gt-李宏毅教授PPT" class="headerlink" title="==&gt;李宏毅教授PPT"></a>==&gt;李宏毅教授PPT</h4><p><img src="/2019/08/16/ReinforenceLearning-Policy-Gradient-Algorithms/AC-3.png" alt="AC-3"></p>
<p><br></p>
<h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3><p>AC是在基于Policy Gradient的基础上进行了变化，融合了Value based的方法.</p>
<p>Actor:就是把$J(\theta)$最大化，但是梯度里面的$Q^{\pi_\theta}(s,a)$用Value based的方法来计算，结果如下：</p>
<p><img src="/2019/08/16/ReinforenceLearning-Policy-Gradient-Algorithms/AC-4.png" alt="AC-4"></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>博客:</p>
<p><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deadly-triad-issue" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deadly-triad-issue</a></p>
<p><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html</a></p>
<p>李宏毅教授：</p>
<p>David_Silver:</p>
<p>配套PPT</p>
<p>Mnih, Volodymyr, et al. <a href="https://arxiv.org/abs/1602.01783" target="_blank" rel="noopener">“Asynchronous methods for deep reinforcement learning.”</a> ICML. 2016.</p>
<p>David Silver, et al. <a href="https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf" target="_blank" rel="noopener">“Deterministic policy gradient algorithms.”</a> ICML. 2014.</p>
<p>Timothy P. Lillicrap, et al. <a href="https://arxiv.org/pdf/1509.02971.pdf" target="_blank" rel="noopener">“Continuous control with deep reinforcement learning.”</a> arXiv preprint arXiv:1509.02971 (2015).</p>

      
      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">Published Date:</span>
          <a href="/2019/08/16/ReinforenceLearning-Policy-Gradient-Algorithms/" class="article-date">
  <time datetime="2019-08-16T06:33:32.000Z" itemprop="datePublished">2019-08-16</time>
</a>

        </li>
        
          <li>
            <span class="label">Category:</span>
            
  <div class="article-category">
    <a class="article-category-link" href="/categories/algorithm/">algorithm</a>
  </div>


          </li>
        
        
          <li>
            <span class="label">Tag:</span>
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>


          </li>
        
        <hr>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <a href="/2019/08/16/hexo-usage/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          hexo-usage
        
      </div>
    </a>
  
  
    <a href="/2019/07/05/python-星号-zip/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">python-星号*, **, zip</div>
    </a>
  
</nav>


  
</article>










      </div>
      
    <footer id="footer" class="post-footer footer">
      
      <hr>
      <div id="footerContent" class="footer-content">
        <p>Do things you want.</p>


      </div>
    </footer>

      







<script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>


<script src="/js/typing.js"></script>
<!--[if lt IE 9]><script src="https://cdn.jsdelivr.net/npm/html5shiv@3/dist/html5shiv.min.js"></script><![endif]-->







    </div>
  </body>
</html>
